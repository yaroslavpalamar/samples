Statement: Design a simplified version of Twitter where people can post tweets, follow other people and favorite tweets.

Questions:
Q1:
how many users do we expect this system to handle? - let’s aim for 10 million users generating around 100 million requests per day

Q2:
That’s great but let’s continue. Since we have the notion of following someone, how connected will these users be? 
After all, they will form a graph with the users being the nodes and the edges will represent who follows whom.
we expect that each user will be following 200 other users on average, but expect some extraordinary users with tens of thousands of followers

Conclusion:
Let’s make a few very simple calculations with the information we just received. 
We will have around 10 million users. Average number of followed other users is 200. 
This means that the network of users will have about 200 * 10 million edges. 
This makes 2 billion edges. If the average number of tweets per day is 10 million the number of favorites will then be 20 million.

To summarize, here are some more things we now know:
10 million users
10 million tweets per day
20 million tweet favorites per day
100 million HTTP requests to the site
2 billion “follow” relations
Some users and tweets could generate an extraordinary amount of traffic

We can divide our architecture in two logical parts: 
1) the logic, which will handle all incoming requests to the application and 
2) the data storage that we will use to store all the data that needs to be persisted. 

application will need to handle requests for:
-posting new tweets
-following a user
-favoriting a tweet
-displaying data about users and tweets

The first three operations require things to be written somewhere in our database, 
while the last is more about reading data and returning it back to the user. 
The last operation will also be the most common

More concrete description:
-There will be a profile page for each user, 
which will show us their latest tweets and will allow for older tweets to be shown.
-Each such page will have a button for following the user. and other buttons
-After they click a button the message will be stored and will appear on their profile page. 
 
___________
We know that the expected daily load is 100 million requests.
This means that on average the app will receive around 1150 requests per second.
-it is good to have more than one server If you only have one machine and it goes down your whole application is down. 
-In our particular problem we would definitely suggest using a load balancer, 
which handles initial traffic and sends requests to a set of servers running one or more instances of the application.
-Behind the load balancer we will be running a set of servers that are running our application and are capable of handling the different requests that arrive.
_________________
Storing the data (count limits based on our preconditions)
We need to store data about our users and their tweets to make the application complete.
Obviously, there are some relations between our data objects - users and tweets. 
Let’s assess the approximate size of the data to be stored. 
We said that we expect around 10 million users. 
For each user we have some profile to store the data 
-Tweets will be generated 10 million per day sot it will be around 115 per second. For a year it will be 3.65 billion per year.
So, let’s aim for a solution that can store efficiently at least 10 billion tweets for now and the incoming traffic mentioned above.
-1 tweet max 140 characters so we should store 140*10billion characters = 1.4 trillion  
  or around 2.8 terabytes if we assume 2 bytes per character(if java and UTF-18) and no compression of the data.
-also there will be connectino between profiles about 2 billiion
  and each connection will most likely just contain two IDs of users where the first follows the second
   it would be enough to store two 4-byte integer fields, making 8 * 2 bln = 16 bln bytes or 16 gigabytes.
-The favorites are expected to grow at a rate of 20 mln per day. So, for a year there will be 7.3 bln such actions. 
  Let’s say we want to be able to store at least 20 bln such objects that poit to one user and one tweet 
  one tweet 8 byte and for user 4 byte so in result will be (4+8)*20 bln so will have 240 gigabytes.
2.6 - 2.7 terabytes. Real-life examples show that famous companies like Twitter and 
Facebook manage to use relational databases for handling much bigger loads than that.
------------------------------------
Database schema - draft the tables and the relations between them?
We have two main entities: users and tweets. There could be two tables for them.
Table users:
-ID (id)
-username (username)
-full name (first_name & last_name)
-password related fields like hash and salt (password_hash & password_salt)
-date of creation and last update (created_at & updated_at)
-description (description)
-and maybe some other fields...

Tweets should be slightly simpler:
Table tweets:
-ID (id)
-content (content)
-date of creation (created_at)
-user ID of author (user_id)

These two entities have several types of relations between them:
users create tweets
users can follow users
users favorite tweets
------------------------------------
Starting with the basics there will be queries for retrieving the details of a given user. Our users’ table above has both id and username fields. 
The next popular query will fetch tweets for a given user. The query needed for doing that will filter tweets using user_id, which every tweet has.
For each user we will want to show the users that they follow and the users that follow them. For this we will need the table connections. To get the users followed by someone we can simply filter the connections table by the column follower_id. To get the users following someone we can filter by followee_id. All this means that it will make a lot of sense to build two indexes in this table - one on follower_id and another one on followee_id. 
---------------
Building a RESTful API
We have a simple plan for what our schema will be like. Another thing that our interviewer could be interested in is how our front-end would “talk” to the back-end system. Probably the most popular answer nowadays would be by exposing a RESTful API on the back-end side, which has a few endpoints returning JSON objects as responses. 
GET /api/users/<username>
GET /api/users/<username>/tweets
GET /api/users/<username>/followers
GET /api/users/<username>/followees
Let’s look at creating new data. For example we will need an endpoint for posting a new tweet:
POST /api/users/<username>/tweets
---------
Increased number of read requests
We have our implementation of the system and it handles everything perfectly. But what would happen if suddenly we got lucky and people started visiting our application 5 times more often generating 5 times more read requests caused by viewing posts and user profiles. What would be the first place that will most likely become a bottleneck?
One very natural answer is our database. It could become overwhelmed with all the read requests coming to it. One typical way to handle more read requests would be to use replication. This way we could increase the number of database instances that hold a copy of the data and allow the application to read this data. 
One of the reasons why using the services of a company like Amazon or Heroku could be beneficial is that they make it easy to add new machines to your environment. You just need to have the money to pay for them.
Finally, if all else is scaled and capable of handling the increased loads we may need to improve our load balancing solution. 
---------
Scaling the database
We just touched on that aspect but let’s talk a bit more about it. Let’s say our application needs to be able to store even more data and the read/write requests per second are increased significantly. If we are using a relational database with the proper indexes running on a single machine it could very quickly become unable to handle the loads that our application experiences. One approach mentioned above is to add an in-memory cache solution in front of the database with the goal to not send repeated read requests to the database itself. We could use a key-value store like memcached to handle that.











